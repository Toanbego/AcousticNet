
@incollection{aytar_soundnet:_2016,
	title = {{SoundNet}: {Learning} {Sound} {Representations} from {Unlabeled} {Video}},
	shorttitle = {{SoundNet}},
	url = {http://papers.nips.cc/paper/6146-soundnet-learning-sound-representations-from-unlabeled-video.pdf},
	urldate = {2019-01-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {892--900},
	file = {NIPS Full Text PDF:C\:\\Users\\toanb\\Zotero\\storage\\AFU7XRXD\\Aytar et al. - 2016 - SoundNet Learning Sound Representations from Unla.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\2PVTATMY\\6146-soundnet-learning-sound-representations-from-unlabeled-video.html:text/html}
}

@article{yu_deep_2011,
	title = {Deep {Learning} and {Its} {Applications} to {Signal} and {Information} {Processing} [{Exploratory} {DSP}]},
	volume = {28},
	issn = {1053-5888},
	doi = {10.1109/MSP.2010.939038},
	abstract = {The purpose of this article is to introduce the readers to the emerging technologies enabled by deep learning and to review the research work conducted in this area that is of direct relevance to signal processing. We also point out, in our view, the future research directions that may attract interests of and require efforts from more signal processing researchers and practitioners in this emerging area for advancing signal and information processing technology and applications.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Yu, D. and Deng, L.},
	month = jan,
	year = {2011},
	keywords = {deep learning, Feature extraction, Hidden Markov models, information processing, Information processing, learning (artificial intelligence), Machine learning, signal processing, Speech recognition, Training},
	pages = {145--154},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\toanb\\Zotero\\storage\\X79ZXTX8\\5670617.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\toanb\\Zotero\\storage\\WTE7G9XR\\Yu and Deng - 2011 - Deep Learning and Its Applications to Signal and I.pdf:application/pdf}
}

@misc{noauthor_practical_nodate,
	title = {Practical {Cryptography}},
	url = {http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/},
	urldate = {2019-01-15},
	file = {Practical Cryptography:C\:\\Users\\toanb\\Zotero\\storage\\6S9QT733\\guide-mel-frequency-cepstral-coefficients-mfccs.html:text/html}
}

@article{baum1966statistical,
  title={Statistical inference for probabilistic functions of finite state Markov chains},
  author={Baum, Leonard E and Petrie, Ted},
  journal={The annals of mathematical statistics},
  volume={37},
  number={6},
  pages={1554--1563},
  year={1966},
  publisher={JSTOR}
}

@misc{matlab_cwtfilterbank,
	title = {cwtfilterbank},
	copyright = {The MathWorks, Inc.},
	url = {https://se.mathworks.com/help/wavelet/ref/cwtfilterbank.html},
	urldate = {2019-04-26},
	month = mar,
	year = {2019},
}


@misc{matlab_classification_scalogram,
	title = {Classify Time Series Using Wavelet Analysis and Deep Learning},
	copyright = {The MathWorks, Inc.},
	url = {https://se.mathworks.com/help/wavelet/examples/signal-classification-with-wavelet-analysis-and-convolutional-neural-networks.html},
	urldate = {2019-04-26},
	month = mar,
	year = {2019},
}


@misc{opencv_geometric_nodate,
	title = {Geometric {Image} {Transformations} - {OpenCV} 2.4.13.7 documentation},
	url = {https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html},
	urldate = {2019-04-26},
	file = {Geometric Image Transformations - OpenCV 2.4.13.7 documentation:C\:\\Users\\toanb\\Zotero\\storage\\SYCGYPA5\\geometric_transformations.html:text/html}
}

@misc{olesen_python_2019,
	title = {Python code for implementing the {Continuous} {Wavelet} {Transform}.: aneergaard/{CWT}},
	copyright = {MIT},
	shorttitle = {Python code for implementing the {Continuous} {Wavelet} {Transform}.},
	url = {https://github.com/aneergaard/CWT},
	urldate = {2019-04-26},
	author = {Olesen, Alexander Neergaard},
	month = mar,
	year = {2019},
	note = {original-date: 2017-08-21T12:45:30Z}
}

@article{hershey_cnn_2016,
	title = {{CNN} {Architectures} for {Large}-{Scale} {Audio} {Classification}},
	url = {http://arxiv.org/abs/1609.09430},
	abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
	urldate = {2019-01-24},
	journal = {arXiv:1609.09430 [cs, stat]},
	author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.09430},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: Accepted for publication at ICASSP 2017 Changes: Added definitions of mAP, AUC, and d-prime. Updated mAP/AUC/d-prime numbers for Audio Set based on changes of latest Audio Set revision. Changed wording to fit 4 page limit with new additions},
	file = {arXiv\:1609.09430 PDF:C\:\\Users\\toanb\\Zotero\\storage\\9U4BDDFP\\Hershey et al. - 2016 - CNN Architectures for Large-Scale Audio Classifica.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\WDIDIATP\\1609.html:text/html}
}

@article{makhoul1980fast,
  title={A fast cosine transform in one and two dimensions},
  author={Makhoul, John},
  journal={IEEE Transactions on Acoustics Speech and Signal Processing},
  volume={28},
  number={1},
  pages={27--34},
  year={1980}
}

@misc{noauthor_scale_nodate,
	title = {Scale to frequency - {MATLAB} scal2frq - {MathWorks} {Nordic}},
	url = {https://se.mathworks.com/help/wavelet/ref/scal2frq.html},
	urldate = {2019-04-13},
	file = {Scale to frequency - MATLAB scal2frq - MathWorks Nordic:C\:\\Users\\toanb\\Zotero\\storage\\Y38ZWDMD\\scal2frq.html:text/html}
}


@Book{Francois_Deep_learning_with_python_optimizer,
  author = 	 {Fran cois Chollet},
  title = 	 {Deep Learning with Python},
  publisher = 	 {Manning Publications},
  year = 	 {2017},
  pages = {54},
  isbn = {9781617294433},
}

@Book{Francois_Deep_learning_with_python_stochastic_decent,
  author = 	 {Francois Chollet},
  title = 	 {Deep Learning with Python},
  publisher = 	 {Manning Publications},
  year = 	 {2017},
  pages = {44--47},
  isbn = {9781617294433},
}




@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2019-04-25},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv\:1412.6980 PDF:C\:\\Users\\toanb\\Zotero\\storage\\AMFMKSEK\\Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@misc{dahal_classification_2017,
	title = {Classification and {Loss} {Evaluation} - {Softmax} and {Cross} {Entropy} {Loss}},
	url = {http://deepnotes.io/softmax-crossentropy},
	abstract = {Lets dig a little deep into how we convert the output of our CNN into probability - Softmax; and the loss measure to guide our optimization - Cross Entropy.},
	language = {en-us},
	urldate = {2019-04-23},
	journal = {DeepNotes},
	author = {Dahal, Paras},
	month = may,
	year = {2017}
}


@misc{pandas_python_nodate,
	title = {Python {Data} {Analysis} {Library} - pandas: {Python} {Data} {Analysis} {Library}},
	url = {https://pandas.pydata.org/},
	urldate = {2019-04-24},
	file = {Python Data Analysis Library - pandas\: Python Data Analysis Library:C\:\\Users\\toanb\\Zotero\\storage\\H22F8NER\\pandas.pydata.org.html:text/html}
}

@article{srivastava_dropout:_nodate,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\toanb\\Zotero\\storage\\JNQJT7B7\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}


@misc{noauthor_continuous_nodate,
	title = {Continuous {Wavelet} {Transform} and {Scale}-{Based} {Analysis} - {MATLAB} \& {Simulink} - {MathWorks} {Nordic}},
	url = {https://se.mathworks.com/help/wavelet/gs/continuous-wavelet-transform-and-scale-based-analysis.html},
	urldate = {2019-04-13},
	file = {Continuous Wavelet Transform and Scale-Based Analysis - MATLAB & Simulink - MathWorks Nordic:C\:\\Users\\toanb\\Zotero\\storage\\UL88RZ8N\\continuous-wavelet-transform-and-scale-based-analysis.html:text/html}
}

@article{cohen2018better,
  title={A better way to define and describe Morlet wavelets for time-frequency analysis},
  author={Cohen, Michael X},
  journal={BioRxiv},
  pages={397182},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}
@article{salehghaffari_speaker_2018,
	title = {Speaker {Verification} using {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1803.05427},
	abstract = {In this paper, a novel Convolutional Neural Network architecture has been developed for speaker verification in order to simultaneously capture and discard speaker and non-speaker information, respectively. In training phase, the network is trained to distinguish between different speaker identities for creating the background model. One of the crucial parts is to create the speaker models. Most of the previous approaches create speaker models based on averaging the speaker representations provided by the background model. We overturn this problem by further fine-tuning the trained model using the Siamese framework for generating a discriminative feature space to distinguish between same and different speakers regardless of their identity. This provides a mechanism which simultaneously captures the speaker-related information and create robustness to within-speaker variations. It is demonstrated that the proposed method outperforms the traditional verification methods which create speaker models directly from the background model.},
	urldate = {2019-01-29},
	journal = {arXiv:1803.05427 [cs, eess]},
	author = {Salehghaffari, Hossein},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.05427},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1803.05427 PDF:C\:\\Users\\toanb\\Zotero\\storage\\A57YUF7U\\Salehghaffari - 2018 - Speaker Verification using Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\SJ8KP7IG\\1803.html:text/html}
}


@article{ahmed1974discrete,
  title={Discrete cosine transform},
  author={Ahmed, Nasir and Natarajan, T\_ and Rao, Kamisetty R},
  journal={IEEE transactions on Computers},
  volume={100},
  number={1},
  pages={90--93},
  year={1974},
  publisher={IEEE}
}
@article{daugman1985uncertainty,
  title={Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters},
  author={Daugman, John G},
  journal={JOSA A},
  volume={2},
  number={7},
  pages={1160--1169},
  year={1985},
  publisher={Optical Society of America}
}
@misc{admin_guide_2018,
	title = {A guide for using the {Wavelet} {Transform} in {Machine} {Learning}},
	url = {http://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/},
	abstract = {[latexpage] 1. Introduction In a previous blog-post we have seen how we can use Signal Processing techniques for the classification of time-series and signals},
	language = {nl},
	urldate = {2019-04-08},
	journal = {Ahmet Taspinar},
	author = {{admin}},
	month = dec,
	year = {2018},
	file = {Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\6Y4VFRAI\\a-guide-for-using-the-wavelet-transform-in-machine-learning.html:text/html}
}

@inproceedings{mcfee2015_augmentation,
    author  = {McFee,, B. and Humphrey,, E.J. and Bello, J.P.},
    year    = {2015},
    title   = {A software framework for musical data augmentation},
    booktitle = {16th International Society for Music Information Retrieval Conference},
    series  = {ISMIR}
}

@article{mohamed2010acoustic,
  title={Acoustic Modeling using Deep Belief Networks},
  author={Mohamed, Abdel-rahman and Dahl, George E and Hinton, Geoffrey},
  year={2010}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}


@article{mohamed_deep_nodate,
	title = {Deep {Neural} {Network} acoustic models for {ASR}},
	language = {en},
	author = {Mohamed, Abdel-rahman},
	pages = {129},
	file = {Mohamed - Deep Neural Network acoustic models for ASR.pdf:C\:\\Users\\toanb\\Zotero\\storage\\PQW922UL\\Mohamed - Deep Neural Network acoustic models for ASR.pdf:application/pdf}
}

@misc{noauthor_fig._nodate,
	title = {Fig. {A}1. {The} standard {VGG}-16 network architecture as proposed in [32]....},
	url = {https://www.researchgate.net/figure/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only_fig3_322512435},
	abstract = {Download scientific diagram {\textbar} Fig. A1. The standard VGG-16 network architecture as proposed in [32]. Note that only layers "conv1" to "fc7" are used in the feature extractor. from publication: Automatic localization of casting defects with convolutional neural networks {\textbar} Automatic localization of defects in metal castings is a challenging task, owing to the rare occurrence and variation in appearance of defects. Convolutional neural networks (CNN) have recently shown outstanding performance in both image classification and localization tasks.... {\textbar} Localization, Defects and Casting {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2019-04-03},
	author = {Max Ferguson},
	journal = {ResearchGate},
	file = {Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\VPTVDFBB\\Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only_fig3_322512435.html:text/html}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{ephrat_looking_2018,
	title = {Looking to {Listen} at the {Cocktail} {Party}: {A} {Speaker}-{Independent} {Audio}-{Visual} {Model} for {Speech} {Separation}},
	volume = {37},
	issn = {07300301},
	shorttitle = {Looking to {Listen} at the {Cocktail} {Party}},
	url = {http://arxiv.org/abs/1804.03619},
	doi = {10.1145/3197517.3201357},
	abstract = {We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
	number = {4},
	urldate = {2019-01-30},
	journal = {ACM Transactions on Graphics},
	author = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
	month = jul,
	year = {2018},
	note = {arXiv: 1804.03619},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--11},
	annote = {Comment: Accepted to SIGGRAPH 2018. Project webpage: https://looking-to-listen.github.io},
	file = {arXiv\:1804.03619 PDF:C\:\\Users\\toanb\\Zotero\\storage\\QDWBMKPP\\Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\XA2Y7KNC\\1804.html:text/html}
}

@inproceedings{mesaros2017dcase,
  title={DCASE 2017 challenge setup: Tasks, datasets and baseline system},
  author={Mesaros, Annamaria and Heittola, Toni and Diment, Aleksandr and Elizalde, Benjamin and Shah, Ankit and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas},
  booktitle={DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events},
  year={2017}
}


@article{standford_lecture5,
	title = {Lecture 5: {Convolutional} {Neural} {Networks}},
	language = {en},
	author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
	pages = {78},
	file = {Li et al. - Lecture 5 Convolutional Neural Networks.pdf:C\:\\Users\\toanb\\Zotero\\storage\\JH72KAXM\\Li et al. - Lecture 5 Convolutional Neural Networks.pdf:application/pdf}
}

@article{standford_lecture6,
	title = {Lecture 6: {Training} {Neural} {Networks}, {Part} {I}},
	language = {en},
	author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
	pages = {89},
	file = {Li et al. - Lecture 6 Training Neural Networks, Part I.pdf:C\:\\Users\\toanb\\Zotero\\storage\\WWBESXPZ\\Li et al. - Lecture 6 Training Neural Networks, Part I.pdf:application/pdf}
}

@misc{google_regularization_nodate,
	title = {Regularization for {Simplicity}: {L2} {Regularization} {\textbar} {Machine} {Learning} {Crash} {Course}},
	shorttitle = {Regularization for {Simplicity}},
	url = {https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization},
	language = {en},
	urldate = {2019-04-04},
	journal = {Google Developers},
	file = {Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\GJTBENNM\\l2-regularization.html:text/html}
}

@book{bellman_dynamic_1957,
	series = {Rand {Corporation} research study},
	title = {Dynamic {Programming}},
	isbn = {978-0-691-07951-6},
	url = {https://books.google.it/books?id=wdtoPwAACAAJ},
	publisher = {Princeton University Press},
	author = {Bellman, R. and Corporation, Rand and Collection, Karreman Mathematics Research},
	year = {1957},
	lccn = {57005444}
}

@misc{noauthor_cs231n_regularization,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/linear-classify/},
	urldate = {2019-04-04},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:C\:\\Users\\toanb\\Zotero\\storage\\7TRYKIZR\\linear-classify.html:text/html}
}

@misc{noauthor_urban_nodate,
	title = {Urban {Sound} {Datasets}},
	url = {http://urbansounddataset.weebly.com/},
	abstract = {Welcome to the companion site for the UrbanSound and UrbanSound8K datasets and the Urban Sound Taxonomy . Here you will find information and download links for the datasets and taxonomy presented in:},
	language = {en},
	urldate = {2019-02-06},
	journal = {Urban Sound Datasets},
	file = {Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\NZZUWH9J\\urbansounddataset.weebly.com.html:text/html}
}

@inproceedings{zhao2018sound,
  title={The sound of pixels},
  author={Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={570--586},
  year={2018}
}


@misc{noauthor_loss_nodate,
	title = {Loss {Functions} - {ML} {Cheatsheet} documentation},
	url = {https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html},
	urldate = {2019-03-29},
	file = {Loss Functions - ML Cheatsheet documentation:C\:\\Users\\toanb\\Zotero\\storage\\TKPRLDZJ\\loss_functions.html:text/html}
}

@book{stephen_marsland_machine_nodate,
	edition = {2},
	title = {Machine {Learning} - {An} {Algorithmic} {Perspective}},
	language = {English},
	publisher = {CRC Press},
	author = {{Stephen Marsland}}
}


@article{zeiler_adadelta:_2012,
	title = {{ADADELTA}: {An} {Adaptive} {Learning} {Rate} {Method}},
	shorttitle = {{ADADELTA}},
	url = {http://arxiv.org/abs/1212.5701},
	abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	urldate = {2019-04-25},
	journal = {arXiv:1212.5701 [cs]},
	author = {Zeiler, Matthew D.},
	month = dec,
	year = {2012},
	note = {arXiv: 1212.5701},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 6 pages},
	file = {arXiv\:1212.5701 PDF:C\:\\Users\\toanb\\Zotero\\storage\\FVK77FY2\\Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\44TKTUFM\\1212.html:text/html}
}

@book{stephen_marsland_machine_nodate_backprop,
	edition = {2},
	title = {Machine {Learning} - {An} {Algorithmic} {Perspective}},
	language = {English},
	publisher = {CRC Press},
	author = {{Stephen Marsland}},
	pages = {44--47}
}


@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}



@misc{raul_gomez_understanding_nodate,
	title = {Understanding {Categorical} {Cross}-{Entropy} {Loss}, {Binary} {Cross}-{Entropy} {Loss}, {Softmax} {Loss}, {Logistic} {Loss}, {Focal} {Loss} and all those confusing names},
	url = {https://gombru.github.io/2018/05/23/cross_entropy_loss/},
	urldate = {2019-03-29},
	author = {{Raul Gomez}},
	file = {Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names:C\:\\Users\\toanb\\Zotero\\storage\\SCQBVAKW\\cross_entropy_loss.html:text/html}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@misc{noauthor_cs231n_activations,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/neural-networks-1/#actfun},
	urldate = {2019-03-27},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:C\:\\Users\\toanb\\Zotero\\storage\\EYNGYEZ4\\neural-networks-1.html:text/html}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}


@misc{standford_index_nodate,
	title = {Index of /slides/2017},
	url = {http://cs231n.stanford.edu/slides/2017/},
	urldate = {2019-04-05},
	file = {Index of /slides/2017:C\:\\Users\\toanb\\Zotero\\storage\\SUGF8G9K\\2017.html:text/html}
}

@article{standford_lecture7,
	title = {Lecture 7: {Training} {Neural} {Networks}, {Part} 2},
	language = {en},
	author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
	pages = {99},
	file = {Li et al. - Lecture 7 Training Neural Networks, Part 2.pdf:C\:\\Users\\toanb\\Zotero\\storage\\PYRFZHNC\\Li et al. - Lecture 7 Training Neural Networks, Part 2.pdf:application/pdf}
}

@misc{noauthor_cs231n_layers,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/convolutional-networks/#layers},
	urldate = {2019-03-27},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:C\:\\Users\\toanb\\Zotero\\storage\\4EEM5ET9\\convolutional-networks.html:text/html}
}

@inproceedings{ren2017deep,
  title={Deep sequential image features on acoustic scene classification},
  author={Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"o}rn},
  booktitle={Proc. DCASE Workshop, Munich, Germany},
  pages={113--117},
  year={2017}
}

@article{chu_environmental_2009,
	title = {Environmental {Sound} {Recognition} {With} {Time}-{Frequency} {Audio} {Features}},
	volume = {17},
	issn = {1558-7916},
	doi = {10.1109/TASL.2009.2017438},
	abstract = {The paper considers the task of recognizing environmental sounds for the understanding of a scene or context surrounding an audio sensor. A variety of features have been proposed for audio recognition, including the popular Mel-frequency cepstral coefficients (MFCCs) which describe the audio spectral shape. Environmental sounds, such as chirpings of insects and sounds of rain which are typically noise-like with a broad flat spectrum, may include strong temporal domain signatures. However, only few temporal-domain features have been developed to characterize such diverse audio signals previously. Here, we perform an empirical feature analysis for audio environment characterization and propose to use the matching pursuit (MP) algorithm to obtain effective time-frequency features. The MP-based method utilizes a dictionary of atoms for feature selection, resulting in a flexible, intuitive and physically interpretable set of features. The MP-based feature is adopted to supplement the MFCC features to yield higher recognition accuracy for environmental sounds. Extensive experiments are conducted to demonstrate the effectiveness of these joint features for unstructured environmental sound classification, including listening tests to study human recognition capabilities. Our recognition system has shown to produce comparable performance as human listeners.},
	number = {6},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Chu, S. and Narayanan, S. and Kuo, C.- J.},
	month = aug,
	year = {2009},
	keywords = {Acoustic noise, Acoustic sensors, Audio classification, audio sensor, audio signal processing, auditory scene recognition, broad flat spectrum, Cepstral analysis, Chirp, data representation, feature extraction, feature selection, human recognition, Humans, Insects, Layout, matching pursuit, matching pursuit algorithm, Matching pursuit algorithms, Mel-frequency cepstral coefficient (MFCC), Mel-frequency cepstral coefficients, pattern recognition, Rain, sound classification, sound recognition, Spectral shape, temporal domain signatures, time-frequency analysis, time-frequency audio features},
	pages = {1142--1158},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\toanb\\Zotero\\storage\\VGA3P8VZ\\5109766.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\toanb\\Zotero\\storage\\DVN3G2NR\\Chu et al. - 2009 - Environmental Sound Recognition With Time-Frequenc.pdf:application/pdf}
}

@misc{noauthor_taxonomy_nodate,
	title = {Taxonomy},
	url = {http://urbansounddataset.weebly.com/taxonomy.html},
	abstract = {The sound classes in the UrbanSound and UrbanSound8K datasets are taken from the urban sound taxonomy presented below. Click here to download a scalable PDF version of the taxonomy . For further...},
	language = {en},
	urldate = {2019-02-06},
	journal = {Urban Sound Datasets},
	file = {Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\9MJ6LN85\\taxonomy.html:text/html}
}

@inproceedings{salamon_dataset_2014,
	address = {Orlando, Florida, USA},
	title = {A {Dataset} and {Taxonomy} for {Urban} {Sound} {Research}},
	isbn = {978-1-4503-3063-3},
	url = {http://dl.acm.org/citation.cfm?doid=2647868.2655045},
	doi = {10.1145/2647868.2655045},
	abstract = {Automatic urban sound classification is a growing area of research with applications in multimedia retrieval and urban informatics. In this paper we identify two main barriers to research in this area - the lack of a common taxonomy and the scarceness of large, real-world, annotated data. To address these issues we present a taxonomy of urban sounds and a new dataset, UrbanSound, containing 27 hours of audio with 18.5 hours of annotated sound event occurrences across 10 sound classes. The challenges presented by the new dataset are studied through a series of experiments using a baseline classification system.},
	language = {en},
	urldate = {2019-02-06},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Multimedia} - {MM} '14},
	publisher = {ACM Press},
	author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},
	year = {2014},
	pages = {1041--1044},
	file = {Salamon et al. - 2014 - A Dataset and Taxonomy for Urban Sound Research.pdf:C\:\\Users\\toanb\\Zotero\\storage\\H5PQ3XV4\\Salamon et al. - 2014 - A Dataset and Taxonomy for Urban Sound Research.pdf:application/pdf}
}

@misc{noauthor_wave_nodate,
	title = {Wave {File} {Specifications}},
	url = {http://www-mmsp.ece.mcgill.ca/Documents/AudioFormats/WAVE/WAVE.html},
	urldate = {2019-02-06},
	file = {Wave File Specifications:C\:\\Users\\toanb\\Zotero\\storage\\EG3WNSZW\\WAVE.html:text/html}
}

@misc{fayek_speech_2016,
	title = {Speech {Processing} for {Machine} {Learning}: {Filter} banks, {Mel}-{Frequency} {Cepstral} {Coefficients} ({MFCCs}) and {What}'s {In}-{Between}},
	shorttitle = {Speech {Processing} for {Machine} {Learning}},
	url = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	abstract = {Understanding and computing filter banks and MFCCs and a discussion on why are filter banks becoming increasingly popular.},
	language = {en},
	urldate = {2019-02-11},
	journal = {Haytham Fayek},
	author = {Fayek, Haytham},
	month = apr,
	year = {2016},
	file = {Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\VHWFAGMK\\speech-processing-for-machine-learning.html:text/html}
}

@article{golchini_new_2013,
	title = {A {New} {Approach} for {Labeling} {Images} {Using} {CAPTCHA} in {Image} {Semantic} {Search} {Engines}},
	volume = {2},
	abstract = {In this paper, we present PICTCHA as a CAPTCHA system (Completely Automated Public Turing test to tell Computers and Humans Apart). This system is a security tool for recognition of human users and instead of a complex text, it uses image labels as a valuable output. PICTCHA is made by Web images. In order to verify this CAPTCHA, users are obliged to enter two words for labeling (naming) a couple of images. In case of convenient names for the images, the presented meaningful names are used for specifying the content of images. Furthermore, meaningful graphics are created for names and images according which we may develop an image semantic search engine. Due to benefiting from images in the proposed system and its architecture, it has higher security level in comparison with other rivals. In experiments with 60 participants, the correctness of PICTCHA words was \%98.18 while about \%61.26 users verified this challenge successfully.},
	language = {en},
	number = {3},
	author = {Golchini, Amir and Haghighat, Abolfazl Toroghi and Rashidi, Hasan},
	year = {2013},
	pages = {8},
	file = {Golchini et al. - 2013 - A New Approach for Labeling Images Using CAPTCHA i.pdf:C\:\\Users\\toanb\\Zotero\\storage\\YMIEKGD7\\Golchini et al. - 2013 - A New Approach for Labeling Images Using CAPTCHA i.pdf:application/pdf}
}

@misc{noauthor_audioset_nodate,
	title = {{AudioSet}},
	url = {https://research.google.com/audioset/dataset/index.html},
	urldate = {2019-02-16},
	file = {AudioSet:C\:\\Users\\toanb\\Zotero\\storage\\FGDNG88P\\index.html:text/html}
}

@inproceedings{munich_bayesian_2004,
	title = {Bayesian subspace methods for acoustic signature recognition of vehicles},
	abstract = {Vehicles may be recognized from the sound they make when moving, i.e., from their acoustic signature. Characteristic patterns may be extracted from the Fourier description of the signature and used for recognition. This paper compares conventional methods used for speaker recognition, namely, systems based on Mel-frequency cepstral coefficients (MFCC) and either Gaussian mixture models (GMM) or hidden Markov models (HMM), with Bayesian subspace method based on the short term Fourier transform (STFT) of the vehicles' acoustic signature. A probabilistic subspace classifier achieves a 11.7\% error for the ACIDS database, outperforming conventional MFCC-GMM- and MFCC-HMM-based systems by 50\%.},
	booktitle = {2004 12th {European} {Signal} {Processing} {Conference}},
	author = {Munich, M. E.},
	month = sep,
	year = {2004},
	keywords = {Hidden Markov models, Mel-frequency cepstral coefficients, Abstracts, ACIDS database, acoustic signal processing, Bayes methods, Bayesian subspace method, characteristic pattern extraction, Fourier description, Fourier transforms, Gaussian mixture models, Gaussian processes, hidden Markov models, Markov processes, MFCC-GMM-based system, MFCC-HMM-based system, mixture models, probabilistic subspace classifier, short-term Fourier transform, speaker recognition, STFT, Topology, vehicle acoustic signature recognition, Vehicles},
	pages = {2107--2110},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\toanb\\Zotero\\storage\\AFRAFUMM\\7080179.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\toanb\\Zotero\\storage\\D9YQXKJQ\\Munich - 2004 - Bayesian subspace methods for acoustic signature r.pdf:application/pdf}
}

@inproceedings{chellappa_vehicle_2004,
	address = {Montreal, Que., Canada},
	title = {Vehicle detection and tracking using acoustic and video sensors},
	volume = {3},
	isbn = {978-0-7803-8484-2},
	url = {http://ieeexplore.ieee.org/document/1326664/},
	doi = {10.1109/ICASSP.2004.1326664},
	abstract = {Multimodal sensing has attracted much attention in solving a wide range of problems, including target detection, tracking, classification, activity understanding, speech recognition, etc. In surveillance applications, different types of sensors, such as video and acoustic sensors, provide distinct observations of ongoing activities. In this paper, we present a fusion framework using both video and acoustic sensors for vehicle detection and tracking. In the detection phase, a rough estimate of target direction-of-arrival (DOA) was first obtained using acoustic data through beam-forming techniques. This initial DOA estimate designates approximate target location in video. Given the initial target position, the DOA is refined by moving target detection using the video data. Markov Chain Monte Carlo techniques are then used for joint audio-visual tracking. A novel fusion approach has been proposed for tracking, based on different characteristics of audio and visual trackers. Experimental results using both synthetic and real data are presented. Improved tracking performance has been observed by fusing the empirical posterior probability density functions obtained using both types of sensors.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Chellappa, R. and {Gang Qian} and {Qinfen Zheng}},
	year = {2004},
	pages = {iii--793--6},
	file = {Chellappa et al. - 2004 - Vehicle detection and tracking using acoustic and .pdf:C\:\\Users\\toanb\\Zotero\\storage\\C26M2WRB\\Chellappa et al. - 2004 - Vehicle detection and tracking using acoustic and .pdf:application/pdf}
}

@inproceedings{chen_deep_2018,
	title = {Deep {Convolutional} {Neural} {Network} with {Scalogram} for {Audio} {Scene} {Modeling}},
	url = {http://www.isca-speech.org/archive/Interspeech_2018/abstracts/1524.html},
	doi = {10.21437/Interspeech.2018-1524},
	abstract = {Deep learning has improved the performance of acoustic scene classification recently. However, learning is usually based on short-time Fourier transform and hand-tailored filters. Learning directly from raw signals has remained a big challenge. In this paper, we proposed an approach to learning audio scene patterns from scalogram, which is extracted from raw signal with simple wavelet transforms. The experiments were conducted on DCASE2016 dataset. We compared scalogram with classical Mel energy, which showed that multi-scale feature led to an obvious accuracy increase. The convolutional neural network integrated with maximum-average downsampled scalogram achieved an accuracy of 90.5\% in the evaluation step in DCASE2016.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Chen, Hangting and Zhang, Pengyuan and Bai, Haichuan and Yuan, Qingsheng and Bao, Xiuguo and Yan, Yonghong},
	month = sep,
	year = {2018},
	pages = {3304--3308},
	file = {Chen et al. - 2018 - Deep Convolutional Neural Network with Scalogram f.pdf:C\:\\Users\\toanb\\Zotero\\storage\\M3FWTTIM\\Chen et al. - 2018 - Deep Convolutional Neural Network with Scalogram f.pdf:application/pdf}
}

@article{munich_bayesian_nodate,
	title = {{BAYESIAN} {SUBSPACE} {METHODS} {FOR} {ACOUSTIC} {SIGNATURE} {RECOGNITION} {OF} {VEHICLES}},
	abstract = {Vehicles may be recognized from the sound they make when moving, i.e., from their acoustic signature. Characteristic patterns may be extracted from the Fourier description of the signature and used for recognition. This paper compares conventional methods used for speaker recognition, namely, systems based on Mel-frequency cepstral coefficients (MFCC) and either Gaussian mixture models (GMM) or hidden Markov models (HMM), with Bayesian subspace method based on the short term Fourier transform (STFT) of the vehicles' acoustic signature. A probabilistic subspace classifier achieves a 11.7\% error for the ACIDS database, outperforming conventional MFCC-GMM- and MFCC-HMM-based systems by 50\%.},
	language = {en},
	author = {Munich, Mario},
	pages = {4},
	file = {Munich - BAYESIAN SUBSPACE METHODS FOR ACOUSTIC SIGNATURE R.pdf:C\:\\Users\\toanb\\Zotero\\storage\\MWGAY8YW\\Munich - BAYESIAN SUBSPACE METHODS FOR ACOUSTIC SIGNATURE R.pdf:application/pdf}
}

@article{davis_comparison_1980,
	title = {Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
	volume = {28},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1980.1163420},
	abstract = {Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.},
	number = {4},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Davis, S. and Mermelstein, P.},
	month = aug,
	year = {1980},
	keywords = {Speech recognition, Speech analysis, Acoustic measurements, Acoustic testing, Band pass filters, Cepstrum, Filtering, Laboratories, Loudspeakers, Nonlinear filters},
	pages = {357--366},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\toanb\\Zotero\\storage\\99FQLILV\\1163420.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\toanb\\Zotero\\storage\\5UURV858\\Davis and Mermelstein - 1980 - Comparison of parametric representations for monos.pdf:application/pdf}
}


@book{homayoon._beigi_fundamentals_2011,
	title = {Fundamentals of {Speaker} {Recognition}},
	publisher = {Springer},
	author = {{Homayoon. Beigi}},
	year = {2011},
	pages = {157--176},
	keywords = {Feature\_extract}
}

@article{salamon_deep_2017,
	title = {Deep {Convolutional} {Neural} {Networks} and {Data} {Augmentation} for {Environmental} {Sound} {Classification}},
	volume = {24},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/1608.04363},
	doi = {10.1109/LSP.2017.2657381},
	abstract = {The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a "shallow" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
	number = {3},
	urldate = {2019-03-11},
	journal = {IEEE Signal Processing Letters},
	author = {Salamon, Justin and Bello, Juan Pablo},
	month = mar,
	year = {2017},
	note = {arXiv: 1608.04363},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	pages = {279--283},
	annote = {Comment: Accepted November 2016, IEEE Signal Processing Letters. Copyright IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material, creating new collective works, for resale or redistribution, or reuse of any copyrighted component of this work in other works},
	file = {arXiv\:1608.04363 PDF:C\:\\Users\\toanb\\Zotero\\storage\\ZLQ5AVGJ\\Salamon and Bello - 2017 - Deep Convolutional Neural Networks and Data Augmen.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\toanb\\Zotero\\storage\\QF2UURPA\\1608.html:text/html}
}

@book{homayoon._beigi_fundamentals_2011-1,
	title = {Fundamentals of {Speaker} {Recognition}},
	publisher = {Springer},
	author = {{Homayoon. Beigi}},
	year = {2011},
	pages = {77--92},
	keywords = {sampling}
}

@misc{noauthor_digital_nodate,
	title = {Digital {Audio} {Resampling} {Home} {Page}},
	url = {https://ccrma.stanford.edu/~jos/resample/},
	urldate = {2019-03-13},
	file = {Digital Audio Resampling Home Page:C\:\\Users\\toanb\\Zotero\\storage\\I3I4BQLD\\resample.html:text/html}
}

@article{dehak_najim_and_kenny_patrick_j_and_dehak_reda_and_dumouchel_pierre_and_ouellet_pierre_front-end_2011,
	title = {Front-{End} {Factor} {Analysis} for {Speaker} {Verification}},
	volume = {19},
	author = {{Dehak, Najim and Kenny, Patrick J and Dehak, R\{{\textbackslash}'e\}da and Dumouchel, Pierre and Ouellet, Pierre}},
	year = {2011}
}

@book{waibel_readings_1990,
	title = {Readings in {Speech} {Recognition}},
	isbn = {978-1-55860-124-6},
	abstract = {After more than two decades of research activity, speech recognition has begun to live up to its promise as a practical technology and interest in the field is growing dramatically. Readings in Speech Recognition provides a collection of seminal papers that have influenced or redirected the field and that illustrate the central insights that have emerged over the years. The editors provide an introduction to the field, its concerns and research problems. Subsequent chapters are devoted to the main schools of thought and design philosophies that have motivated different approaches to speech recognition system design. Each chapter includes an introduction to the papers that highlights the major insights or needs that have motivated an approach to a problem and describes the commonalities and differences of that approach to others in the book.},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Waibel, Alexander and Lee, Kai-Fu},
	month = may,
	year = {1990},
	note = {Google-Books-ID: yjzCra5eW3AC},
	keywords = {Computers / Intelligence (AI) \& Semantics, Science / General}
}